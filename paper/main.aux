\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hinton2015distilling}
\citation{touvron2023llama}
\citation{zhang2024tinyllama}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{hinton2015distilling}
\citation{romero2015fitnets}
\citation{zagoruyko2017paying}
\citation{sanh2019distilbert}
\citation{jiao2020tinybert}
\citation{sun2020mobilebert}
\citation{touvron2023llama}
\citation{zhang2024tinyllama}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Knowledge Distillation.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feature-Based Distillation.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distillation for Transformers.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LLM Compression.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Models}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher.}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Student.}{2}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Loss Function}{2}{subsection.3.2}\protected@file@percent }
\citation{wang2018glue}
\citation{hendrycks2021measuring}
\citation{cobbe2021training}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Datasets}{3}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset statistics}}{3}{table.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:datasets}{{1}{3}{Dataset statistics}{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Training Configuration}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overall Performance}{3}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Overall accuracy (final epoch, $N=7$ seeds per method). Statistical significance vs. black-box: ** $p < 0.01$. Best results in \textbf  {bold}.}}{4}{table.caption.8}\protected@file@percent }
\newlabel{tab:main_results}{{2}{4}{Overall accuracy (final epoch, $N=7$ seeds per method). Statistical significance vs. black-box: ** $p < 0.01$. Best results in \textbf {bold}}{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Task-Specific Results}{4}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Task-specific accuracy breakdown. Attention distillation provides substantial gains on language understanding (SST-2, MMLU) but minimal improvement on mathematical reasoning (GSM8K).}}{4}{figure.caption.9}\protected@file@percent }
\newlabel{fig:task_breakdown}{{1}{4}{Task-specific accuracy breakdown. Attention distillation provides substantial gains on language understanding (SST-2, MMLU) but minimal improvement on mathematical reasoning (GSM8K)}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Learning Dynamics}{4}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curves showing validation accuracy over training epochs. Shaded regions indicate $\pm $1 standard deviation across 7 seeds.}}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:learning_curves}{{2}{5}{Learning curves showing validation accuracy over training epochs. Shaded regions indicate $\pm $1 standard deviation across 7 seeds}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Why Does Attention Work Better?}{5}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Why Doesn't Combining Signals Help?}{5}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Task-Dependent Benefits}{5}{subsection.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Validation loss curves. Attention-based methods achieve approximately half the final loss of black-box distillation.}}{6}{figure.caption.11}\protected@file@percent }
\newlabel{fig:loss_curves}{{3}{6}{Validation loss curves. Attention-based methods achieve approximately half the final loss of black-box distillation}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Practical Implications}{6}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{6}{section.6}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\bibcite{cobbe2021training}{{1}{2021}{{Cobbe et~al.}}{{Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.}}}
\bibcite{hendrycks2021measuring}{{2}{2021}{{Hendrycks et~al.}}{{Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}}}
\bibcite{hinton2015distilling}{{3}{2015}{{Hinton et~al.}}{{Hinton, Vinyals, and Dean}}}
\bibcite{jiao2020tinybert}{{4}{2020}{{Jiao et~al.}}{{Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and Liu}}}
\bibcite{romero2015fitnets}{{5}{2015}{{Romero et~al.}}{{Romero, Ballas, Kahou, Chassang, Gatta, and Bengio}}}
\bibcite{sanh2019distilbert}{{6}{2019}{{Sanh et~al.}}{{Sanh, Debut, Chaumond, and Wolf}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{7}{section.7}\protected@file@percent }
\bibcite{sun2020mobilebert}{{7}{2020}{{Sun et~al.}}{{Sun, Yu, Song, Liu, Yang, and Zhou}}}
\bibcite{touvron2023llama}{{8}{2023}{{Touvron et~al.}}{{Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.}}}
\bibcite{wang2018glue}{{9}{2018}{{Wang et~al.}}{{Wang, Singh, Michael, Hill, Levy, and Bowman}}}
\bibcite{zagoruyko2017paying}{{10}{2017}{{Zagoruyko and Komodakis}}{{}}}
\bibcite{zhang2024tinyllama}{{11}{2024}{{Zhang et~al.}}{{Zhang, Zheng, Liu, Lu, Chang, Wang, Zhang, Tang, and He}}}
\gdef \@abspage@last{8}
