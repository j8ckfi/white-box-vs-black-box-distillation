\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}

\title{Attention Alignment Outperforms Logit Distillation for LLM Compression: \\
A Comparative Study of White-Box vs Black-Box Knowledge Transfer}

\author{
  Jack Large\textsuperscript{1} \and Madelyn Sarbin\textsuperscript{1} \\
  \textsuperscript{1}Independent Researcher
}

\date{}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Knowledge distillation enables compressing large language models (LLMs) into smaller, deployable variants. 
While standard ``black-box'' distillation transfers only output logits, ``white-box'' methods additionally align internal representations such as hidden states and attention maps. 
We systematically compare four distillation approaches for compressing Llama-2-7B into TinyLlama-1.1B across sentiment analysis (SST-2), reasoning (MMLU), and mathematical problem-solving (GSM8K) tasks.

Our experiments ($N=7$ seeds per method) reveal that \textbf{attention alignment achieves significantly higher accuracy} (95.56\% vs 93.98\%, $p < 0.01$), representing a \textbf{26\% reduction in error rate}. 
Notably, combining attention with hidden state alignment yields no additional benefit, suggesting attention maps are the primary driver of improvement. 
White-box methods also exhibit substantially lower variance (1.43\% vs 2.98\% std), indicating more stable training dynamics.

These findings demonstrate that for heterogeneous teacher-student pairs with matched attention head counts, attention distillation provides a simple, effective enhancement over standard knowledge distillation with minimal computational overhead during training.
\end{abstract}

% ============================================================================
% INTRODUCTION
% ============================================================================
\section{Introduction}

Large language models (LLMs) have achieved remarkable performance across diverse NLP tasks, but their size---often billions of parameters---poses significant deployment challenges. Knowledge distillation \citep{hinton2015distilling} offers a promising compression approach by training a smaller ``student'' model to mimic a larger ``teacher'' model.

Traditional distillation operates in a ``black-box'' manner, transferring only the teacher's output logits. However, LLMs encode rich intermediate representations: hidden states capture semantic features, while attention maps encode structural relationships between tokens. Can transferring these internal signals---``white-box'' distillation---improve student performance?

We investigate this question by comparing four distillation methods:
\begin{enumerate}
    \item \textbf{Black-Box}: Standard KL-divergence on output logits
    \item \textbf{Hidden State}: Logits + final-layer hidden state alignment
    \item \textbf{Attention}: Logits + final-layer attention map alignment
    \item \textbf{Combined}: All signals (logits + hidden states + attention)
\end{enumerate}

Our experiments compress Llama-2-7B \citep{touvron2023llama} into TinyLlama-1.1B \citep{zhang2024tinyllama}---a 6.4$\times$ compression ratio. We evaluate on three diverse tasks: sentiment classification (SST-2), multi-task reasoning (MMLU), and grade-school mathematics (GSM8K).

Our key finding: \textbf{attention alignment provides statistically significant improvement} ($p < 0.01$) over black-box distillation, reducing errors by 26\% (from 6.02\% to 4.44\% error rate). Hidden state alignment alone offers modest gains (+0.73\%). Combining signals yields no additional benefit over attention-only, suggesting attention maps are the dominant transfer signal for heterogeneous LLM distillation.

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}

\paragraph{Knowledge Distillation.}
\citet{hinton2015distilling} introduced knowledge distillation, showing that soft probability distributions from a teacher network provide richer supervision than hard labels. This approach has been extensively applied to model compression in NLP.

\paragraph{Feature-Based Distillation.}
FitNets \citep{romero2015fitnets} extended distillation to intermediate representations, training students to match teacher hidden states. This ``hint-based'' training accelerates convergence and improves generalization. Subsequent work explored attention transfer \citep{zagoruyko2017paying}, demonstrating that attention maps encode valuable structural information.

\paragraph{Distillation for Transformers.}
DistilBERT \citep{sanh2019distilbert} successfully applied distillation to BERT, achieving 97\% of BERT's performance with 60\% fewer parameters. TinyBERT \citep{jiao2020tinybert} further incorporated embedding-layer and attention-based distillation, while MobileBERT \citep{sun2020mobilebert} demonstrated effective distillation for resource-constrained devices.

\paragraph{LLM Compression.}
Recent work has explored distillation for modern LLMs, though most studies focus on logit-based approaches. Our work systematically evaluates whether white-box signals provide benefits for heterogeneous LLM distillation, where teacher and student architectures differ significantly.

% ============================================================================
% METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Models}

\paragraph{Teacher.} Llama-2-7B \citep{touvron2023llama}: 7 billion parameters, 4096-dimensional hidden states, 32 attention heads.

\paragraph{Student.} TinyLlama-1.1B \citep{zhang2024tinyllama}: 1.1 billion parameters (6.4$\times$ smaller), 2048-dimensional hidden states, 32 attention heads.

\subsection{Loss Function}

The total training loss combines multiple components:
\begin{equation}
\mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{task}} + \beta \mathcal{L}_{\text{KD}} + \gamma_1 \mathcal{L}_{\text{hidden}} + \gamma_2 \mathcal{L}_{\text{attn}}
\end{equation}

where:
\begin{itemize}
    \item $\mathcal{L}_{\text{task}}$: Cross-entropy on ground-truth labels ($\alpha = 1.0$)
    \item $\mathcal{L}_{\text{KD}}$: KL-divergence between teacher and student logits ($\beta = 0.5$)
    \item $\mathcal{L}_{\text{hidden}}$: MSE between projected student and teacher hidden states ($\gamma_1 = 0.1$)
    \item $\mathcal{L}_{\text{attn}}$: MSE between teacher and student attention maps ($\gamma_2 = 0.1$)
\end{itemize}

For hidden state alignment, we use a trainable linear projector to map student representations (2048-dim) to match teacher dimensions (4096-dim). Attention maps require no projection since both models have 32 heads.

\subsection{Datasets}

We evaluate on three diverse tasks (Table~\ref{tab:datasets}):
\begin{itemize}
    \item \textbf{SST-2} \citep{wang2018glue}: Binary sentiment classification (5,000 examples)
    \item \textbf{MMLU} \citep{hendrycks2021measuring}: Multi-task reasoning across domains (1,000 examples)
    \item \textbf{GSM8K} \citep{cobbe2021training}: Grade-school math word problems (1,000 examples)
\end{itemize}

\begin{table}[h]
\centering
\caption{Dataset statistics}
\label{tab:datasets}
\begin{tabular}{llll}
\toprule
Dataset & Task Type & Examples & Evaluation \\
\midrule
SST-2 & Sentiment (NLU) & 5,000 & Binary accuracy \\
MMLU & Reasoning & 1,000 & Multi-choice accuracy \\
GSM8K & Math & 1,000 & Exact match \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Configuration}

All experiments use: learning rate $10^{-4}$ (AdamW), batch size 8, 3 epochs, max sequence length 512, gradient clipping at 1.0. Each method is run with 7 random seeds (0--6) for statistical validity. Teacher outputs are pre-computed offline for efficiency.

% ============================================================================
% RESULTS
% ============================================================================
\section{Results}

\subsection{Overall Performance}

Table~\ref{tab:main_results} presents the main results. Attention-based distillation achieves the highest accuracy at 95.56\%, outperforming the black-box baseline by +1.58 percentage points. This difference is statistically significant (two-sample $t$-test, $t = 3.44$, $p < 0.01$). In terms of error reduction, attention distillation reduces the error rate from 6.02\% to 4.44\%---a \textbf{26\% relative reduction in errors}.

The combined approach matches attention-only performance exactly, suggesting hidden states provide no additional signal when attention alignment is present.

\begin{table}[h]
\centering
\caption{Overall accuracy (final epoch, $N=7$ seeds per method). Statistical significance vs. black-box: ** $p < 0.01$. Best results in \textbf{bold}.}
\label{tab:main_results}
\begin{tabular}{lcccc}
\toprule
Method & Mean Accuracy & Std Dev & vs Black-Box & Error Reduction \\
\midrule
\textbf{Attention}** & \textbf{95.56\%} & 1.43\% & \textbf{+1.58\%} & \textbf{26\%} \\
\textbf{Combined}** & \textbf{95.56\%} & 1.43\% & \textbf{+1.58\%} & \textbf{26\%} \\
Hidden State & 94.71\% & 1.05\% & +0.73\% & 12\% \\
Black-Box & 93.98\% & 2.98\% & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Task-Specific Results}

Figure~\ref{fig:task_breakdown} shows performance across individual tasks. Attention distillation provides the largest gain on SST-2 (+2.40\%), followed by MMLU (+0.79\%). Notably, the improvement on GSM8K is minimal (+0.09\%), suggesting that attention alignment primarily benefits language understanding tasks rather than structured mathematical reasoning.

This pattern is consistent with the hypothesis that attention maps encode linguistic relationships (e.g., sentiment-bearing words, contextual dependencies) that transfer well between architectures, while mathematical reasoning may depend more on learned computational patterns within hidden states that do not transfer as effectively.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../figures/task_breakdown.pdf}
\caption{Task-specific accuracy breakdown. Attention distillation provides substantial gains on language understanding (SST-2, MMLU) but minimal improvement on mathematical reasoning (GSM8K).}
\label{fig:task_breakdown}
\end{figure}

\subsection{Learning Dynamics}

Figure~\ref{fig:learning_curves} shows validation accuracy over training epochs. White-box methods converge to higher final accuracy and exhibit lower variance (shaded regions). Black-box distillation shows notably higher seed-to-seed variation (2.98\% std vs 1.43\%), indicating less stable training dynamics.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../figures/learning_curves.pdf}
\caption{Learning curves showing validation accuracy over training epochs. Shaded regions indicate $\pm$1 standard deviation across 7 seeds.}
\label{fig:learning_curves}
\end{figure}

Validation loss (Figure~\ref{fig:loss_curves}) confirms this pattern: attention-based methods achieve substantially lower final loss (0.028) compared to black-box (0.056).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../figures/loss_curves.pdf}
\caption{Validation loss curves. Attention-based methods achieve approximately half the final loss of black-box distillation.}
\label{fig:loss_curves}
\end{figure}

% ============================================================================
% DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Why Does Attention Work Better?}

We hypothesize that attention maps encode \textit{structural} information---which tokens attend to which---that transcends the specific representational capacity of the model. Unlike hidden states, which encode semantic features in model-specific vector spaces, attention patterns may represent relational structure that transfers more naturally between architectures.

TinyLlama and Llama-2 share the same number of attention heads (32), enabling direct alignment without projection. In contrast, hidden state alignment requires learning a linear transformation from 2048 to 4096 dimensions, which may introduce representational bottlenecks. However, we note that this explanation is speculative; further analysis (e.g., probing experiments, attention visualization) would be needed to confirm this hypothesis.

\subsection{Why Doesn't Combining Signals Help?}

The identical performance of Attention and Combined methods suggests that once attention alignment is present, hidden state alignment provides redundant or conflicting supervision. We hypothesize that the attention objective already constrains the model sufficiently that additional hidden state matching adds no benefit---or potentially introduces competing gradients.

\subsection{Task-Dependent Benefits}

The near-zero improvement on GSM8K (+0.09\%) compared to substantial gains on SST-2 (+2.40\%) suggests that attention alignment is most beneficial for tasks requiring nuanced language understanding. Mathematical reasoning may rely more on learned computational patterns within hidden representations rather than attention structure. This finding suggests practitioners should consider task characteristics when choosing distillation strategies.

\subsection{Practical Implications}

For practitioners compressing LLMs with matched attention head counts:
\begin{enumerate}
    \item \textbf{Use attention distillation}: Provides statistically significant improvement ($p < 0.01$) with 26\% error reduction
    \item \textbf{Skip hidden state alignment}: Adds complexity without measurable benefit
    \item \textbf{Expect more stable training}: Lower variance across random seeds
    \item \textbf{Consider task type}: Benefits are larger for language understanding than mathematical reasoning
\end{enumerate}

% ============================================================================
% LIMITATIONS
% ============================================================================
\section{Limitations}

\begin{itemize}
    \item \textbf{Single model pair}: Our results are specific to Llama-2-7B $\rightarrow$ TinyLlama-1.1B. Generalization to other teacher-student pairs (e.g., Mistral $\rightarrow$ smaller models) requires additional experiments, which we leave to future work.
    \item \textbf{Matched attention heads}: Both models have 32 attention heads, enabling direct alignment. Mismatched configurations would require attention projection, which may reduce or eliminate benefits.
    \item \textbf{Single layer alignment}: We align only final-layer attention; multi-layer alignment may provide additional benefits.
    \item \textbf{Task scope}: Three tasks provide initial evidence; broader evaluation across more diverse benchmarks is warranted.
    \item \textbf{Fixed hyperparameters}: Loss weights ($\gamma_1, \gamma_2$) were fixed at 0.1; tuning may affect relative rankings.
\end{itemize}

% ============================================================================
% CONCLUSION
% ============================================================================
\section{Conclusion}

We systematically compared white-box and black-box knowledge distillation for LLM compression. Our experiments demonstrate that attention alignment provides statistically significant improvement ($p < 0.01$, 26\% error reduction) over standard logit distillation when compressing Llama-2-7B to TinyLlama-1.1B, while also reducing training variance. The benefit is strongest for language understanding tasks (SST-2, MMLU) and minimal for mathematical reasoning (GSM8K). Hidden state alignment provides modest benefits alone but no additional gains when combined with attention distillation.

These findings suggest that for heterogeneous LLM distillation with matched attention heads, practitioners should prioritize attention alignment over hidden state matching, particularly for language-focused tasks. Future work should explore multi-layer attention alignment and investigate whether these findings generalize to other model architectures.

% ============================================================================
% REPRODUCIBILITY
% ============================================================================
\section*{Reproducibility}

Code is available at: \url{https://github.com/j8ckfi/white-box-vs-black-box-distillation}. All experiments use publicly available models (Llama-2-7B, TinyLlama-1.1B) and datasets (SST-2, MMLU, GSM8K). Training was conducted on NVIDIA GPUs with PyTorch and Hugging Face Transformers. Full hyperparameters are documented in Section 3.4.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
